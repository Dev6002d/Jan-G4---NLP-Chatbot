{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyamsparrow/Jan-G4---NLP-Chatbot/blob/main/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DqqzSMEsAnw"
      },
      "source": [
        "# Standard libraries\n",
        "import re\n",
        "import string\n",
        "from unicodedata import normalize\n",
        "from typing import List, Optional, Union, Callable\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from spellchecker import SpellChecker\n",
        "#from names_dataset import NameDataset\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "class PreProcessing:\n",
        "  def __init__(self, convert_lowercase = False,\n",
        "               remove_num=False,\n",
        "               remove_whitespaces=False,\n",
        "               remove_spl_char=False,\n",
        "               remove_stopwords=False):\n",
        "    self.convert_lowercase = convert_lowercase\n",
        "    self.remove_num=remove_num\n",
        "    self.remove_whitespaces=remove_whitespaces\n",
        "    self.remove_spl_char=remove_spl_char\n",
        "  \n",
        "  def preprocess(self, text):\n",
        "\n",
        "    if self.convert_lowercase:\n",
        "      text = to_lower(text)\n",
        "\n",
        "    if self.remove_num:\n",
        "      text = remove_num_method(text)\n",
        "    \n",
        "    if self.remove_spl_char:\n",
        "      text = remove_spl_char_method(text)\n",
        "    \n",
        "    if self.remove_whitespaces:\n",
        "      text = self.remove_wspace(text)\n",
        "\n",
        "    return text\n",
        "  def to_lower(self,text):\n",
        "    \"\"\" Convert input text to lower case \"\"\"\n",
        "    return text.lower()\n",
        "\n",
        "  def remove_number(self,text):\n",
        "    \"\"\" Remove number in the input text \"\"\"\n",
        "    processed_text = re.sub('\\d+', '', text)\n",
        "    return processed_text\n",
        "\n",
        "  def remove_url(self,text):\n",
        "    \"\"\" Remove url in the input text \"\"\"\n",
        "    return re.sub('(www|http)\\S+', '', text)\n",
        "\n",
        "  def remove_punctuation(self,text):\n",
        "    \"\"\"\n",
        "    Removes all punctuations from a string, as defined by string.punctuation or a custom list.\n",
        "    For reference, Python's string.punctuation is equivalent to '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
        "    \"\"\"\n",
        "    punctuations = string.punctuation\n",
        "    processed_text = text.translate(str.maketrans('', '', punctuations))\n",
        "    return processed_text\n",
        "\n",
        "  def remove_special_character(self,text):\n",
        "    \"\"\" Removes special characters \"\"\"\n",
        "    special_characters = 'å¼«¥ª°©ð±§µæ¹¢³¿®ä£'\n",
        "    processed_text = text.translate(str.maketrans('', '', special_characters))\n",
        "    return processed_text\n",
        "\n",
        "  def keep_alpha_numeric(self,text):\n",
        "    \"\"\" Remove any character except alphanumeric characters \"\"\"\n",
        "    return ''.join(c for c in text if c.isalnum())\n",
        "\n",
        "  def remove_whitespace(self,text, remove_duplicate_whitespace: bool = True):\n",
        "    \"\"\" Removes leading, trailing, and (optionally) duplicated whitespace \"\"\"\n",
        "    if remove_duplicate_whitespace:\n",
        "        return ' '.join(re.split('\\s+', input_text.strip(), flags=re.UNICODE))\n",
        "    return input_text.strip()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ounU-FnlD7"
      },
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import logging\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from functools import wraps\n",
        "from unicodedata import normalize\n",
        "from typing import List, Optional, Union, Callable\n",
        "\n",
        "# Third party libraries\n",
        "#import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "from spellchecker import SpellChecker\n",
        "#from names_dataset import NameDataset\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "_CUSTOM_SUB_CSV_FILE_PATH = os.path.join(os.path.dirname(__file__), 'data/custom_substitutions.csv')\n",
        "_IGNORE_SPELLCHECK_WORD_FILE_PATH = os.path.join(os.path.dirname(__file__), 'data/ignore_spellcheck_words.txt')\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "LOGGER.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "def _return_empty_string_for_invalid_input(func):\n",
        "    \"\"\" Return empty string if the input is None or empty \"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        if 'input_text' in kwargs:\n",
        "            input_text = kwargs['input_text']\n",
        "        else:\n",
        "            try:\n",
        "                input_text = args[0]\n",
        "            except IndexError as e:\n",
        "                LOGGER.exception('No appropriate positional argument is provide.')\n",
        "                raise e\n",
        "        if input_text is None or len(input_text) == 0:\n",
        "            return ''\n",
        "        else:\n",
        "            return func(*args, **kwargs)\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def _return_empty_list_for_invalid_input(func):\n",
        "    \"\"\" Return empty list if the input is None or empty \"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        if 'input_text_or_list' in kwargs:\n",
        "            input_text_or_list = kwargs['input_text_or_list']\n",
        "        else:\n",
        "            try:\n",
        "                input_text_or_list = args[0]\n",
        "            except IndexError as e:\n",
        "                LOGGER.exception('No appropriate positional argument is provide.')\n",
        "                raise e\n",
        "        if input_text_or_list is None or len(input_text_or_list) == 0:\n",
        "            return []\n",
        "        else:\n",
        "            return func(*args, **kwargs)\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def to_lower(input_text: str) -> str:\n",
        "    \"\"\" Convert input text to lower case \"\"\"\n",
        "    return input_text.lower()\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def to_upper(input_text: str) -> str:\n",
        "    \"\"\" Convert input text to upper case \"\"\"\n",
        "    return input_text.upper()\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_number(input_text: str) -> str:\n",
        "    \"\"\" Remove number in the input text \"\"\"\n",
        "    processed_text = re.sub('\\d+', '', input_text)\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_itemized_bullet_and_numbering(input_text: str) -> str:\n",
        "    \"\"\" Remove bullets or numbering in itemized input \"\"\"\n",
        "    processed_text = re.sub('[(\\s][0-9a-zA-Z][.)]\\s+|[(\\s][ivxIVX]+[.)]\\s+', ' ', input_text)\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_url(input_text: str) -> str:\n",
        "    \"\"\" Remove url in the input text \"\"\"\n",
        "    return re.sub('(www|http)\\S+', '', input_text)\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_punctuation(input_text: str, punctuations: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Removes all punctuations from a string, as defined by string.punctuation or a custom list.\n",
        "    For reference, Python's string.punctuation is equivalent to '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
        "    \"\"\"\n",
        "    if punctuations is None:\n",
        "        punctuations = string.punctuation\n",
        "    processed_text = input_text.translate(str.maketrans('', '', punctuations))\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_special_character(input_text: str, special_characters: Optional[str] = None) -> str:\n",
        "    \"\"\" Removes special characters \"\"\"\n",
        "    if special_characters is None:\n",
        "        # TODO: add more special characters\n",
        "        special_characters = 'å¼«¥ª°©ð±§µæ¹¢³¿®ä£'\n",
        "    processed_text = input_text.translate(str.maketrans('', '', special_characters))\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def keep_alpha_numeric(input_text: str) -> str:\n",
        "    \"\"\" Remove any character except alphanumeric characters \"\"\"\n",
        "    return ''.join(c for c in input_text if c.isalnum())\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_whitespace(input_text: str, remove_duplicate_whitespace: bool = True) -> str:\n",
        "    \"\"\" Removes leading, trailing, and (optionally) duplicated whitespace \"\"\"\n",
        "    if remove_duplicate_whitespace:\n",
        "        return ' '.join(re.split('\\s+', input_text.strip(), flags=re.UNICODE))\n",
        "    return input_text.strip()\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def expand_contraction(input_text: str) -> str:\n",
        "    \"\"\" Expand contractions in input text \"\"\"\n",
        "    return contractions.fix(input_text)\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def normalize_unicode(input_text: str) -> str:\n",
        "    \"\"\" Normalize unicode data to remove umlauts, and accents, etc. \"\"\"\n",
        "    processed_tokens = normalize('NFKD', input_text).encode('ASCII', 'ignore').decode('utf8')\n",
        "    return processed_tokens\n",
        "\n",
        "\n",
        "@_return_empty_list_for_invalid_input\n",
        "def remove_stopword(input_text_or_list: Union[str, List[str]], stop_words: Optional[set] = None) -> List[str]:\n",
        "    \"\"\" Remove stop words \"\"\"\n",
        "\n",
        "    if stop_words is None:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "    if isinstance(stop_words, list):\n",
        "        stop_words = set(stop_words)\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "        processed_tokens = [token for token in tokens if token not in stop_words]\n",
        "    else:\n",
        "        processed_tokens = [token for token in input_text_or_list\n",
        "                            if (token not in stop_words and token is not None and len(token) > 0)]\n",
        "    return processed_tokens\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_email(input_text: str) -> str:\n",
        "    \"\"\" Remove email in the input text \"\"\"\n",
        "    regex_pattern = '[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}'\n",
        "    return re.sub(regex_pattern, '', input_text)\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_phone_number(input_text: str) -> str:\n",
        "    \"\"\" Remove phone number in the input text \"\"\"\n",
        "    regex_pattern = '(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})(?: *x(\\d+))?'\n",
        "    return re.sub(regex_pattern, '', input_text)\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_ssn(input_text: str) -> str:\n",
        "    \"\"\" Remove social security number in the input text \"\"\"\n",
        "    regex_pattern = '(?!219-09-9999|078-05-1120)(?!666|000|9\\d{2})\\d{3}-(?!00)\\d{2}-(?!0{4})\\d{4}|(' \\\n",
        "                    '?!219099999|078051120)(?!666|000|9\\d{2})\\d{3}(?!00)\\d{2}(?!0{4})\\d{4}'\n",
        "    return re.sub(regex_pattern, '', input_text)\n",
        "\n",
        "\n",
        "@_return_empty_string_for_invalid_input\n",
        "def remove_credit_card_number(input_text: str) -> str:\n",
        "    \"\"\" Remove credit card number in the input text \"\"\"\n",
        "    regex_pattern = '(4[0-9]{12}(?:[0-9]{3})?|(?:5[1-5][0-9]{2}|222[1-9]|22[3-9][0-9]|2[3-6][0-9]{2}|27[01][' \\\n",
        "                    '0-9]|2720)[0-9]{12}|3[47][0-9]{13}|3(?:0[0-5]|[68][0-9])[0-9]{11}|6(?:011|5[0-9]{2})[0-9]{12}|(' \\\n",
        "                    '?:2131|1800|35\\d{3})\\d{11})'\n",
        "    return re.sub(regex_pattern, '', input_text)\n",
        "\n",
        "\n",
        "@_return_empty_list_for_invalid_input\n",
        "def remove_name(input_text_or_list: Union[str, List[str]]) -> List[str]:\n",
        "    \"\"\" Remove name in the input text \"\"\"\n",
        "    name_searcher = NameDataset()\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "        processed_tokens = [token for token in tokens\n",
        "                            if (not name_searcher.search_first_name(token)) and\n",
        "                               (not name_searcher.search_last_name(token))]\n",
        "    else:\n",
        "        processed_tokens = [token for token in input_text_or_list\n",
        "                            if (not name_searcher.search_first_name(token)) and\n",
        "                               (not name_searcher.search_last_name(token)) and token is not None and len(token) > 0]\n",
        "    return processed_tokens\n",
        "\n",
        "\n",
        "def check_spelling(input_text_or_list: Union[str, List[str]], lang='en',\n",
        "                   ignore_word_file_path: Union[str, Path] = _IGNORE_SPELLCHECK_WORD_FILE_PATH) -> str:\n",
        "    \"\"\" Check and correct spellings of the text list \"\"\"\n",
        "    if input_text_or_list is None or len(input_text_or_list) == 0:\n",
        "        return ''\n",
        "    spelling_checker = SpellChecker(language=lang, distance=1)\n",
        "    # TODO: add acronyms into spell checker to ignore auto correction specified by _IGNORE_SPELLCHECK_WORD_FILE_PATH\n",
        "    spelling_checker.word_frequency.load_text_file(ignore_word_file_path)\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        if not input_text_or_list.islower():\n",
        "            input_text_or_list = input_text_or_list.lower()\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "    else:\n",
        "        tokens = [token.lower() for token in input_text_or_list if token is not None and len(token) > 0]\n",
        "    misspelled = spelling_checker.unknown(tokens)\n",
        "    for word in misspelled:\n",
        "        tokens[tokens.index(word)] = spelling_checker.correction(word)\n",
        "    return ' '.join(tokens).strip()\n",
        "\n",
        "\n",
        "def tokenize_word(input_text: str) -> List[str]:\n",
        "    \"\"\" Converts a text into a list of word tokens \"\"\"\n",
        "    if input_text is None or len(input_text) == 0:\n",
        "        return []\n",
        "    return word_tokenize(input_text)\n",
        "\n",
        "\n",
        "def tokenize_sentence(input_text: str) -> List[str]:\n",
        "    \"\"\" Converts a text into a list of sentence tokens \"\"\"\n",
        "    if input_text is None or len(input_text) == 0:\n",
        "        return []\n",
        "    tokenizer = PunktSentenceTokenizer()\n",
        "    return tokenizer.tokenize(input_text)\n",
        "\n",
        "\n",
        "@_return_empty_list_for_invalid_input\n",
        "def stem_word(input_text_or_list: Union[str, List[str]],\n",
        "              stemmer: Optional[Union[PorterStemmer, SnowballStemmer, LancasterStemmer]] = None\n",
        "              ) -> List[str]:\n",
        "    \"\"\" Stem each token in a text \"\"\"\n",
        "    if stemmer is None:\n",
        "        stemmer = PorterStemmer()\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "        processed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    else:\n",
        "        processed_tokens = [stemmer.stem(token) for token in input_text_or_list if token is not None and len(token) > 0]\n",
        "    return processed_tokens\n",
        "\n",
        "\n",
        "@_return_empty_list_for_invalid_input\n",
        "def lemmatize_word(input_text_or_list: Union[str, List[str]],\n",
        "                   lemmatizer: Optional[WordNetLemmatizer] = None\n",
        "                   ) -> List[str]:\n",
        "    \"\"\" Lemmatize each token in a text by finding its base form \"\"\"\n",
        "    if lemmatizer is None:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    else:\n",
        "        processed_tokens = [lemmatizer.lemmatize(token)\n",
        "                            for token in input_text_or_list if token is not None and len(token) > 0]\n",
        "    return processed_tokens\n",
        "\n",
        "\n",
        "def substitute_token(token_list: List[str], sub_dict: Optional[dict] = None) -> List[str]:\n",
        "    \"\"\" Substitute each token by another token, e.g., 'vs' -> 'versus' \"\"\"\n",
        "    # TODO: add more custom substitutions in the csv file specified by _CUSTOM_SUB_CSV_FILE_PATH\n",
        "    if token_list is None or len(token_list) == 0:\n",
        "        return []\n",
        "    if sub_dict is None:\n",
        "        with open(_CUSTOM_SUB_CSV_FILE_PATH, 'r') as f:\n",
        "            csv_file = csv.reader(f)\n",
        "            sub_dict = dict(csv_file)\n",
        "    processed_tokens = list()\n",
        "    for token in token_list:\n",
        "        if token in sub_dict:\n",
        "            processed_tokens.append(sub_dict[token])\n",
        "        else:\n",
        "            processed_tokens.append(token)\n",
        "    return processed_tokens\n",
        "\n",
        "\n",
        "def preprocess_text(input_text: str, processing_function_list: Optional[List[Callable]] = None) -> str:\n",
        "    \"\"\" Preprocess an input text by executing a series of preprocessing functions specified in functions list \"\"\"\n",
        "    if processing_function_list is None:\n",
        "        processing_function_list = [to_lower,\n",
        "                                    remove_url,\n",
        "                                    check_spelling,\n",
        "                                    remove_special_character,\n",
        "                                    remove_punctuation,\n",
        "                                    remove_whitespace,\n",
        "                                    remove_stopword,\n",
        "                                    substitute_token,\n",
        "                                    lemmatize_word]\n",
        "    for func in processing_function_list:\n",
        "        input_text = func(input_text)\n",
        "    if isinstance(input_text, str):\n",
        "        processed_text = input_text\n",
        "    else:\n",
        "        processed_text = ' '.join(input_text)\n",
        "    return processed_text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id1Kw0htzohc",
        "outputId": "f27e007a-bbd9-4652-f0d9-16e6fc935bf5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0hlcsqnsn7W",
        "outputId": "b0e57eea-65b2-49ad-b9a0-75a983bf5d8f"
      },
      "source": [
        "%%writefile config.py\n",
        "remove_num=True\n",
        "remove_spl_char = True\n",
        "remove_whitespaces = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC_8CARUtWfR"
      },
      "source": [
        "import config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4fHUI2EtXuf",
        "outputId": "146afe58-2b60-4c47-ab2b-2d312ff1072a"
      },
      "source": [
        "config.remove_whitespaces"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "FKsNOEvatCcb",
        "outputId": "81b47b36-d65e-4064-e06a-d0f2eb53a378"
      },
      "source": [
        "pp = PreProcessing(remove_whitespaces=config.remove_whitespaces)\n",
        "\n",
        "pp.preprocess(\"this is a sample text    \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is a sample text    '"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}