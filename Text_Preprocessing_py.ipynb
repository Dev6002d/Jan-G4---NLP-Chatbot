{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing.py",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyamsparrow/Jan-G4---NLP-Chatbot/blob/main/Text_Preprocessing_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8P19nMNdq5T",
        "outputId": "8f552d45-413f-4606-eedc-47e0c409ffcf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxv9FPQsgSuV",
        "outputId": "2cb99aa8-d51a-461b-9df0-d0eccc5c7b9e"
      },
      "source": [
        "%cd /content/MyDrive/AIML/Capstone"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/MyDrive/AIML/Capstone'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fa2J7svdgNb",
        "outputId": "8eefda21-33f9-4d37-846e-605f086ccfc8"
      },
      "source": [
        "! git clone https://github.com/shyamsparrow/Jan-G4---NLP-Chatbot.git\n",
        "! git pull"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Jan-G4---NLP-Chatbot' already exists and is not an empty directory.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBVf4UNcfvuU",
        "outputId": "aa1e0955-4b81-42b0-aa4a-1c5c10fa64f8"
      },
      "source": [
        "import sys\n",
        "print(os.getcwd())\n",
        "print(sys.argv[0])\n",
        "print(os.path.dirname(os.path.realpath('__file__')))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JO-HrNned0So",
        "outputId": "0e90f858-db87-468f-d65a-bb4b82dd91f1"
      },
      "source": [
        "_CUSTOM_SUB_CSV_FILE_PATH = os.path.join(os.getcwd(), 'Jan-G4---NLP-Chatbot/data/custom_substitutions.csv')\n",
        "_CUSTOM_SUB_CSV_FILE_PATH"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Jan-G4---NLP-Chatbot/data/custom_substitutions.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DqqzSMEsAnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee9da05-8c53-4471-ecfd-0d826d30cdbb"
      },
      "source": [
        "# Standard libraries\n",
        "!pip install pyspellchecker\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import logging\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from functools import wraps\n",
        "from unicodedata import normalize\n",
        "from typing import List, Optional, Union, Callable\n",
        "\n",
        "# Third party libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "_CUSTOM_SUB_CSV_FILE_PATH = os.path.join(os.path.dirname(__file__), 'data/custom_substitutions.csv')\n",
        "_IGNORE_SPELLCHECK_WORD_FILE_PATH = os.path.join(os.getcwd(), 'data/ignore_spellcheck_words.txt')\n",
        "\n",
        "\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "LOGGER.setLevel(logging.INFO)\n",
        "class PreProcessing:\n",
        "  def __init__(self, to_lower = False,remove_url=False,remove_special_character=False, remove_punctuation=False,\n",
        "               remove_whitespace=False,check_spelling=False,remove_stopword=False,substitute_token=False,lemmatize_word=False,\n",
        "               tokenize_word=False, tokenize_sentence= False):\n",
        "    \n",
        "    self.to_lower = to_lower\n",
        "    self.remove_url=remove_url\n",
        "    self.remove_special_character=remove_special_character\n",
        "    self.remove_punctuation=remove_punctuation\n",
        "    self.remove_whitespace=remove_whitespace\n",
        "    self.check_spelling=check_spelling\n",
        "    self.remove_stopword=remove_stopword\n",
        "    self.substitute_token=substitute_token\n",
        "    self.lemmatize_word=lemmatize_word\n",
        "    self.tokenize_word=tokenize_word\n",
        "    self.tokenize_sentence=tokenize_sentence\n",
        "  \n",
        "  def preprocess(self, input_text):\n",
        "\n",
        "    if self.to_lower:\n",
        "      input_text = self.to_lower_method(input_text)\n",
        "\n",
        "    if self.remove_url:\n",
        "      input_text = self.remove_url_method(input_text)\n",
        "    \n",
        "    if self.remove_special_character:\n",
        "      input_text = self.remove_special_character_method(input_text)\n",
        "    \n",
        "    if self.remove_punctuation:\n",
        "      input_text = self.remove_punctuation_method(input_text)\n",
        "\n",
        "    if self.remove_whitespace:\n",
        "      input_text = self.remove_whitespace_method(input_text)\n",
        "      \n",
        "    if self.check_spelling:\n",
        "      input_text = self.check_spelling_method(input_text)\n",
        "      \n",
        "    if self.remove_stopword:\n",
        "      input_text = self.remove_stopword_method(input_text)\n",
        "      \n",
        "    if self.substitute_token:\n",
        "      input_text = self.substitute_token_method(input_text)\n",
        "      \n",
        "    if self.lemmatize_word:\n",
        "      input_text = self.lemmatize_word_method(input_text)\n",
        "            \n",
        "    if self.tokenize_word:\n",
        "      input_text = self.tokenize_word_method(input_text)\n",
        "            \n",
        "    if self.tokenize_sentence:\n",
        "      input_text = self.tokenize_sentence_method(input_text)\n",
        "    \n",
        "    if isinstance(input_text, str):\n",
        "        processed_text = input_text\n",
        "    else:\n",
        "        processed_text = ' '.join(input_text)\n",
        "    return processed_text\n",
        "\n",
        "    return input_text\n",
        "\n",
        "\n",
        "  def to_lower_method(self,input_text:str)->str:\n",
        "    \"\"\" Convert input text to lower case \"\"\"\n",
        "    return input_text.lower()\n",
        "\n",
        "  def remove_url_method(self,input_text:str)-> str:\n",
        "    \"\"\" Remove url in the input text \"\"\"\n",
        "    return re.sub('(www|http)\\S+', '', input_text)\n",
        "\n",
        "  \n",
        "  def remove_punctuation_method(self,input_text:str)-> str:\n",
        "    \"\"\"\n",
        "    Removes all punctuations from a string, as defined by string.punctuation or a custom list.\n",
        "    For reference, Python's string.punctuation is equivalent to '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~'\n",
        "    \"\"\"\n",
        "    punctuations = string.punctuation\n",
        "    processed_text = input_text.translate(str.maketrans('', '', punctuations))\n",
        "    return processed_text\n",
        "\n",
        "  def remove_special_character_method(self,input_text:str)-> str:\n",
        "    \"\"\" Removes special characters \"\"\"\n",
        "    special_characters = 'å¼«¥ª°©ð±§µæ¹¢³¿®ä£'\n",
        "    processed_text = input_text.translate(str.maketrans('', '', special_characters))\n",
        "    return processed_text\n",
        "\n",
        "  def keep_alpha_numeric_method(self,input_text:str):\n",
        "    \"\"\" Remove any character except alphanumeric characters \"\"\"\n",
        "    return ''.join(c for c in input_text if c.isalnum())\n",
        "\n",
        "  def remove_whitespace_method(self,input_text:str, remove_duplicate_whitespace: bool = True)-> str:\n",
        "    \"\"\" Removes leading, trailing, and (optionally) duplicated whitespace \"\"\"\n",
        "    if remove_duplicate_whitespace:\n",
        "        return ' '.join(re.split('\\s+', input_text.strip(), flags=re.UNICODE))\n",
        "    return input_text.strip()\n",
        "\n",
        "  def remove_stopword_method(self,input_text_or_list: Union[str, List[str]])-> List[str]:\n",
        "    \"\"\" Remove stop words \"\"\"\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    if isinstance(stop_words, list):\n",
        "        stop_words = set(stop_words)\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "        processed_tokens = [token for token in tokens if token not in stop_words]\n",
        "    else:\n",
        "        processed_tokens = [token for token in input_text_or_list\n",
        "                            if (token not in stop_words and token is not None)]\n",
        "    return processed_tokens\n",
        "     \n",
        "  def lemmatize_word_method(self,input_text_or_list: Union[str, List[str]])-> List[str]:\n",
        "    \"\"\" Lemmatize each token in a text by finding its base form \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    else:\n",
        "        processed_tokens = [lemmatizer.lemmatize(token) for token in input_text_or_list if token is not None]\n",
        "    return processed_tokens\n",
        "     \n",
        "  def tokenize_word_method(self,input_text: str) -> List[str]:\n",
        "    \"\"\" Converts a text into a list of word tokens \"\"\"\n",
        "    if input_text is None:\n",
        "        return []\n",
        "    return word_tokenize(input_text)\n",
        "\n",
        "\n",
        "  def tokenize_sentence_method(self,input_text: str) -> List[str]:\n",
        "    \"\"\" Converts a text into a list of sentence tokens \"\"\"\n",
        "    if input_text is None:\n",
        "        return []\n",
        "    tokenizer = PunktSentenceTokenizer()\n",
        "    return tokenizer.tokenize(input_text)\n",
        "\n",
        "  def substitute_token_method(self,token_list: List[str])-> List[str]:\n",
        "    \"\"\" Substitute each token by another token, e.g., 'vs' -> 'versus' \"\"\"\n",
        "    # TODO: add more custom substitutions in the csv file specified by _CUSTOM_SUB_CSV_FILE_PATH\n",
        "    if token_list is None:\n",
        "        return []\n",
        "    with open(_CUSTOM_SUB_CSV_FILE_PATH, 'r') as f:\n",
        "            csv_file = csv.reader(f)\n",
        "            sub_dict = dict(csv_file)\n",
        "    processed_tokens = list()\n",
        "\n",
        "    for token in token_list:\n",
        "        if token in sub_dict:\n",
        "            processed_tokens.append(sub_dict[token])\n",
        "        else:\n",
        "            processed_tokens.append(token)\n",
        "    return processed_tokens\n",
        "\n",
        "  def check_spelling_method(self,input_text_or_list: Union[str, List[str]], lang='en',\n",
        "                   ignore_word_file_path: Union[str, Path] = _IGNORE_SPELLCHECK_WORD_FILE_PATH) -> str:\n",
        "    \"\"\" Check and correct spellings of the text list \"\"\"\n",
        "    if input_text_or_list is None:\n",
        "        return ''\n",
        "    spelling_checker = SpellChecker()\n",
        "    # TODO: add acronyms into spell checker to ignore auto correction specified by _IGNORE_SPELLCHECK_WORD_FILE_PATH\n",
        "    #spelling_checker.word_frequency.load_text_file(ignore_word_file_path)\n",
        "    if isinstance(input_text_or_list, str):\n",
        "        if not input_text_or_list.islower():\n",
        "            input_text_or_list = input_text_or_list.lower()\n",
        "        tokens = word_tokenize(input_text_or_list)\n",
        "    else:\n",
        "        tokens = [token.lower() for token in input_text_or_list if token is not None]\n",
        "    misspelled = spelling_checker.unknown(tokens)\n",
        "    for word in misspelled:\n",
        "        tokens[tokens.index(word)] = spelling_checker.correction(word)\n",
        "    return ' '.join(tokens).strip()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id1Kw0htzohc",
        "outputId": "1f5a88c3-b4af-42a0-b9fb-da5020963cc0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0hlcsqnsn7W",
        "outputId": "f6a718b6-68ef-4739-f4cf-f9e504f1b835"
      },
      "source": [
        "%%writefile config.py\n",
        "to_lower = True\n",
        "remove_url=True\n",
        "remove_special_character=True\n",
        "remove_punctuation=True\n",
        "remove_whitespace=True\n",
        "check_spelling=True\n",
        "remove_stopword=True\n",
        "substitute_token=True\n",
        "lemmatize_word=True\n",
        "tokenize_word=True\n",
        "tokenize_sentence= False"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC_8CARUtWfR"
      },
      "source": [
        "import config"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4fHUI2EtXuf",
        "outputId": "13a970b9-e760-4089-cc5a-9933c6964c0c"
      },
      "source": [
        "config.tokenize_sentence check_spelling=config.check_spelling"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "FKsNOEvatCcb",
        "outputId": "4ca804bc-953b-434d-db6a-f13e4b3c5989"
      },
      "source": [
        "pp = PreProcessing(to_lower = config.to_lower, remove_url=config.remove_url, remove_special_character=config.remove_special_character, \n",
        "                   remove_punctuation=config.remove_punctuation, remove_whitespace=config.remove_whitespace,\n",
        "                   check_spelling = config.check_spelling, remove_stopword=config.remove_stopword, \n",
        "                   substitute_token=config.substitute_token, lemmatize_word=config.lemmatize_word,\n",
        "                   tokenize_word=config.tokenize_word, tokenize_sentence= config.tokenize_sentence)\n",
        "\n",
        "text_to_process = 'Helllo,    I am John Doe!!! My email is john.doe@email.com. Visit our website www.johndoe.com'\n",
        "\n",
        "pp.preprocess(text_to_process)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-99afc0eb7ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext_to_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Helllo,    I am John Doe!!! My email is john.doe@email.com. Visit our website www.johndoe.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-bec84d87b773>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstitute_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m       \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstitute_token_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-bec84d87b773>\u001b[0m in \u001b[0;36msubstitute_token_method\u001b[0;34m(self, token_list)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CUSTOM_SUB_CSV_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0msub_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/custom_substitutions.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iUdwPiMm012G",
        "outputId": "943135b4-ba25-4c22-9cbd-d1908a7a706b"
      },
      "source": [
        "import os\n",
        "\n",
        "_CUSTOM_SUB_CSV_FILE_PATH = os.path.join(os.getcwd(), 'data/custom_substitutions.csv')\n",
        "_IGNORE_SPELLCHECK_WORD_FILE_PATH = os.path.join(os.getcwd(), 'data/ignore_spellcheck_words.txt')\n",
        "\n",
        "_CUSTOM_SUB_CSV_FILE_PATH"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/data/custom_substitutions.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYt7b0vrcXF1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}